{
  "hash": "ccc654a8c49a6ec01b92d7e183f181ff",
  "result": {
    "markdown": "---\ntitle: \"Final Project Assignment #2: Tim Shores\"\nauthor: \"Tim Shores\"\ndescription: \"Project & Data Description\"\ndate: \"04/09/2023\"\nformat:\n  html:\n    df-print: paged\n    toc: true\n    code-copy: true\n    code-tools: true\n    code-fold: true\n    code-overflow: wrap\n    css: styles.css\ncategories:\n  - final_Project_assignment_1\n  - final_project_data_description\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_packages <- c(\"tidyverse\", \"fs\", \"pdftools\", \"knitr\") # create vector of packages\ninvisible(lapply(my_packages, require, character.only = TRUE)) # load multiple packages\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n```\n:::\n\n\n### Summary\n\n300 words go here.\n\nThe important part: What is the nature of the public safety need behind the increase in calls?\n\n\n### Background of the Topic\n\nSince 2020, I have volunteered my time as a community organizer and political office-holder in the small rural town of Leverett, Massachusetts. Some of the key challenges of public administration in Leverett are a high property tax rate,   unaffordability, the needs of an aging population, lack of transportation, lack of access to commerce and health service, lack of access to public process, and lack of resources for gathering good evidence for public decision-making.\n\nAt the recent budget meetings in Spring 2023, the Leverett Police Chief made a budget request for a fourth full-time officer position. This position would help the Police Department overcome these problems:\n\n1. Decreased supply of police officer human resources due to state criminal justice reform.\n2. Increased demand for police officer human resources due to increased call volume in the towns of Leverett and Wendell.\n\n::: {.callout-tip icon=false}\n\n## Problem #1 in detail\n\n**How has state reform decreased supply of police officers?**\n\n:::\n\nSince 2021, state POST reform has made it difficult for local police departments to fill their on-duty schedules with reserve police officers. Before 2021, reserve officers required half as much training as full-time officers. Reserve officers could work part-time and as a second job in seasonal positions or on shifts as-needed could do so with lower training requirements. The State of Massachusetts Peace Officer Standards and Training (POST) Commission has enacted a series of reforms that increased the amount of training required by reserve officers. Reserves must now train as much as full-time officers. This has removed an incentive and reduced the availability of reserve officers. Local police departments now struggle to meet their scheduling needs and turn to requests for more full-time positions.\n\n::: {.callout-tip icon=false}\n\n## Problem #2 in detail\n\n**How much and why has call volume increased in Leverett and Wendell?**\n\n:::\n\nSince 2020, when the Wendell chief of police retired, the neighboring towns of Leverett and Wendell have agreed to combine their municipal police service. Wendell closed its own police department (WPD), and began to allocate its police budget revenue to the Leverett Police Department (LPD). LPD took responsibility for responding to police and emergency calls from both Leverett and Wendell. This regionalization of municipal service is expected to reduce public safety costs over time, although in the short-term it appears that Leverett has paid for more police expenses compared to Wendell's share. Meanwhile, LPD is still adjusting to the change in demand for police service, which shows up as an increase in call volume that officers respond to.  \n\nTo solve these problems, Leverett Chief of Police Scott Minckler proposed that we hire a fourth full-time officer. \n\nThe request was approved by the Leverett and Wendell Police Services Oversight committee, the Leverett Personnel Board, and the Leverett Finance Committee. When the matter came before the Leverett Selectboard budget meeting, some members of town government and the public wanted to know more detail about the increase in call volume. \n\n::: {.callout-tip icon=false}\n\n## Questions of public interest\n\na. What types of calls are increasing? \nb. What is the change in personnel time needed per call and per type of call?\nc. What days and times of day do we see the most calls or the most change in calls?\nd. Why do we need another officer when the populations and number of properties in the two towns have not grown substantially?\n\n:::\n\nChief Minckler's responses to these questions were informed by his administrative experience, but not by call data analysis. Due to difficulties generating reports from the administrative software used by many emergency response departments, the Chief was not able to use call data to do more than track call volume and call type. The Chief was not able to answer questions about the nature of increased calls: Were they violent crime, domestic violence, interpersonal conflict, fraud, property rights violations, traffic violations, medical emergencies, mental health emergencies, fires? \n\nAlthough people had multiple concerns and interests in the matter, most meeting participants had this basic question in common: \n\n::: {.callout-warning icon=false}\n\n## The top question\n\n**What is the nature of the public safety need behind the increase in calls?**\n\n:::\n\nIn other words, if we hire another police officer, will that meet the public safety need? If our increased needs are by nature medical, mental health, fire, or community health and well-being, then **we risk misallocating public resources in a solution to the wrong problem**.\n\n\n### Dataset Introduction\nTo collect data that could help understand more about the public safety need of Leverett and Wendell, I submitted this public records request: \n\n::: {.callout-note icon=false}\n\n## Email to PD: Thu, Mar 30, 2023 at 3:00 PM\n\nSubject: Public Records Request for Leverett PD call data\nTo: Scott D. Minckler <policechief@leverett.ma.us>\n\nHi Chief Minckler,\n\nI also left a voicemail, but thought it would help to send my request by email. I've been learning about the budget requests from last week's meeting to get a sense of the request and the call volume and any other factors behind the request. It would help if I could learn more about our call data. \n\nI'm writing to request public data to do with Leverett and Wendell calls made to dispatch, and calls handled by our police department, for the 5-year period 2018 to 2022 (inclusive of the years 2018, 2019, 2020, 2021, and 2022). I'm requesting complete police department call detail records, with columns of data redacted if they are subject to specific statutory exemption to the MA Public Records Law due to criteria such as victim confidentiality. \n\nPlease export data from your call management software application to CSV or XLSX files, rather than PDF or Word documents. The tabular CSV or XLSX format will facilitate data analysis.  \n\nIf the export files are too large for email attachment, please upload to this Drive location: \n*url omitted*\n\nThanks! Let me know if you have any questions or feedback.\nTim Shores\n\n:::\n\nChief Minckler called me the following Monday morning to discuss the request. He could supply reports, but not to the level of detail that I requested. I learned more about the challenges of pulling reports. I also learned from Chief Minckler that one reason for the growth of calls in Wendell was that the former police chief of Wendell did not record every call. This was not made known at the time when the two towns negotiated their regionalization agreement. \n\nI spoke with Officer Charles \"Butch\" Garrity at Shelburne Control Dispatch Center, which serves as the 911 dispatcher for regional towns including Leverett and Wendell, and which supports a digital call-management network operated with a client-server application called Central Square, formerly known as IMC. Chief Minckler and Officer Garrity agree that Central Square is difficult to use and limited in functionality:\n\n::: {.callout-warning icon=false}\n\n### Source report limitations\n\n1. The report export format was limited to PDF.\n2. They could only provide call summary data. It was not practical to provide call detail data that would include relevant information (such as call day and time of day, location, demographics of people involved, and details of actions taken by responding officers) because Central Square has no redaction functions. \n:::\n\nBased on my career experience with numerous database systems, I'm confident that technical means exist to overcome these functional limits. However, I take the claims of the LPD and Shelburne Dispatch to be made in good faith. At the end of the day, it comes down to a lack of resources, and as a researcher I must set my expectations accordingly.\n\nChief Minckler sent me 10 PDF files: one for each town and year, from 2018 to 2022. Each file includes 3 tables:\n\n::: {.callout-note icon=false}\n\n## Source report structure\n\n1. **Call Reason Breakdown** shows the number of calls for each call reason, average time per call per call reason, and the number of action taken for each call reason.\n2. **Call Action Breakdown** shows summaries of call actions.\n3. **Operator Race and Sex Breakdown** shows demographics of vehicle operators involved in traffic stops and violations.\n:::\n\n**Call Reason Breakdown** is the primary object of this analysis.\n\nI used the R libraries `purrr` and `pdf_tools` to read-in the data to a list of 10 lists of data frames: one data frame for each page, one list for each file, in a collection of 10 files.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprojectFiles <- dir_ls('../posts/TimShores_FinalProject_Data') # creates fs_path vector of each file in that directory\n\nprojectFiles_list <- projectFiles %>%\n  map(pdf_data) # purrr function that applies pdf_data reading function to each file element of fs_path vector\n  # map creates a list of lists of tibbles (a tibble for each page of each pdf)\n\n  # now that I've consumed my files I can str_replace to get town and year info from each filename.\n\nprojectFiles <- projectFiles %>% \n  str_replace('../posts/TimShores_FinalProject_Data/', '') %>%\n  str_replace('.pdf', '') \nnames(projectFiles_list) <- projectFiles\n```\n:::\n\n\nI took the following general steps to parse this list:\n\n::: {.callout-note icon=false}\n\n## Data transformation steps\n\n1. Mutated each row with information about the town, year, and source page number.\n2. Flattened the nested list of data frames into a single data frame of all observations that retained the horizontal and vertical arrangement of the tables in the source reports.\n3. When pdf_data reads a file, it parses each text box encoded in the PDF as a separate data frame row. It records x and y coordinates of each text box in a data frame variable. Given the regularity of horizontal and vertical alignment of tables in the Central Square report format, this made it possible to identify column boundaries: each column had a minimum and maximum value for x. I used `pivot_wider` to create new variables to temporarily store values based on their horizontal position on the page.\n4. The Call Reason Breakdown table in the source reports is a combination of call reasons (each call has a single reason) and call actions (each call has 0, 1, or many actions taken by officers). I focused most of my tidying and transformation steps to teasing apart Call Reason Breakdown observations into separate data frames: one for reasons, another for actions.\n5. I created a third data frame for operator demographics, although the reports do not show how this data is related to calls.\n6. My code removed all NA values. Every observation in the original reports had values for every variable. However, because the Call Reason Breakdown table format staggers data about call reasons and actions, the initial data frame had many NAs. When plyering apart the semantics of reason and action, I interpret NA to indicate that I must remove the row or fill in the variable with a sub-heading value. The end result is three data frames with no NAs.\n:::\n\nSee my comments in the code block below for details.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  # This next part took a couple days of searching for a tidy solution. \n  # Then I just decided to use a good old-fashioned nested for loop.\n  # The structure of the original documents is that each of 10 files has a variable number of pages.\n  # Therefore, the projectFiles_list structure is a nested list of dataframes: list[[list]][[dataframe]]. \n  # I want to mutate each df with the page number from its PDF, or \"innerpageid\". \n  # Inner pagination corresponds to each dataframe's nested list element index (j in list[[i]][[j]]) which \n  # is information that gets destroyed as soon as I map to an unnested list. I couldn't find a tidy solution to capturing that nested\n  # \"inner\" page id before the map, so I used a nested for loop to assign value to innerpageid before appending my mapped/mutated df to\n  # a new single-layer list.\n\nallYearsList <- list()\nfor (i in seq_along(projectFiles_list)) { # seq_along(projectFiles_list) creates vector with a number value for each list element\n  for (j in seq_along(projectFiles_list[[i]])) {\n    projectFiles_list[[i]][[j]] <- mutate(projectFiles_list[[i]][[j]], innerpageid = j)\n  }\n  allYearsList <- append(allYearsList, map2(projectFiles_list[[i]], \n                                            names(projectFiles_list[i]), \n                                            ~.x %>% \n                                              mutate(filename = .y)\n                                            )\n                         )\n  }\n\n  # this bind_rows takes all of the df observations from all of the list elements and binds them together in a single df.\n  # outerpageid is the 'page' number in an absolute sequence (1 to 78 in this case) rather than segmented by file (1 to 9, 1 to 7 etc.)\n  # Note that outerpageid is not particularly important (it's semantically identical to town, year, innerpageid) \n  # and I should remove this scaffold.\n\nallYearsDF <- allYearsList %>%\n  bind_rows(.id = 'outerpageid') %>% \n  relocate(c(filename, innerpageid), .after = outerpageid) %>%\n  filter(height == 11 & y > 13 & text != \"Action:\")\n  \n  # this creates a problem of staggered lines\n\nallYearsDF_wide <- allYearsDF %>% \n  pivot_wider(id_cols = c(outerpageid, filename, innerpageid, y), names_from = x, values_from = text)\n\n# solution from https://stackoverflow.com/questions/45515218/combine-rows-in-data-frame-containing-na-to-make-complete-row\n# Supply lists by splicing them into dots:\n\ncoalesce_by_column <- function(df) {\n  return(dplyr::coalesce(!!! as.list(df)))\n}\n\nallYearsDF_wide <- allYearsDF_wide %>%\n  mutate(outerpageid = as.integer(outerpageid))\n\nallYearsDF_wide <- allYearsDF_wide %>%\n  group_by(outerpageid, filename, innerpageid, y) %>% # outerpageid is redundant here\n  summarise_all(coalesce_by_column) %>%\n  ungroup()\n# do I need to ungroup() ? doesn't seem to hurt!\n\n  # it will help to order columns by numeric order - they are the x coords from left to right. This looks for all non-alpha column names.\n\nnumcols = sort(as.numeric(names(allYearsDF_wide)[!grepl(\"[a-z]\", names(allYearsDF_wide))]))\nallYearsDF_wide <- select(allYearsDF_wide, outerpageid, filename, innerpageid, y, match(numcols, names(allYearsDF_wide))) \n\n  # now we have a df that is simply x:y coordinates of the page\n  # time to smush cols together!\n\nallYearsDF_wide <- unite(allYearsDF_wide, \"x5to52\", c(\"5\":\"47\"), sep = \" \", na.rm = TRUE, remove = TRUE)\nallYearsDF_wide <- unite(allYearsDF_wide, \"x53to197\", c(\"53\":\"197\"), sep = \" \", na.rm = TRUE, remove = TRUE)\nallYearsDF_wide <- unite(allYearsDF_wide, \"x198to238\", c(\"198\":\"233\"), sep = \" \", na.rm = TRUE, remove = TRUE)\nallYearsDF_wide <- unite(allYearsDF_wide, \"x239to280\", c(\"239\":\"270\"), sep = \" \", na.rm = TRUE, remove = TRUE)\nallYearsDF_wide <- unite(allYearsDF_wide, \"x281to329\", c(\"281\":\"329\"), sep = \" \", na.rm = TRUE, remove = TRUE)\nallYearsDF_wide <- unite(allYearsDF_wide, \"x330to371\", c(\"330\":\"371\"), sep = \" \", na.rm = TRUE, remove = TRUE)\nallYearsDF_wide <- unite(allYearsDF_wide, \"x372to449\", c(\"372\":\"431\"), sep = \" \", na.rm = TRUE, remove = TRUE)\nallYearsDF_wide <- unite(allYearsDF_wide, \"x450toEnd\", c(\"450\":\"545\"), sep = \" \", na.rm = TRUE, remove = TRUE)\n\n  # note that spacing does differ on each page (as indicated by the colnames), but the boundaries are stable\n  # (as indicated by the united colnames -- I manually confirmed the minimum pixel position of each column on each data frame)\n\n  # final tidying of Call_Reason text separated into different columns\n\nallYearsDF_wide <- mutate(allYearsDF_wide, \n                          x5to52 = ifelse(nchar(x5to52, type=\"chars\") > 0 & nchar(x53to197, type=\"chars\") > 0, \n                                          paste0(x5to52,\" \",x53to197), \n                                          paste0(x5to52)),\n                          x53to197 = ifelse(nchar(x5to52, type=\"chars\") > 0, \n                                            paste0(\"\"), \n                                            paste0(x53to197))) # delete from additional columns\n\nallYearsDF_wide <- mutate(allYearsDF_wide, \n                          x53to197 = ifelse(x53to197 == \"\", paste0(\"\"), paste0(x53to197, \" \", x198to238, \" \", x239to280, \" \", x281to329, \" \", x330to371, \" \", x372to449, \" \", x450toEnd)),\n                          x198to238 = ifelse(nchar(x53to197, type=\"chars\") > 0, paste0(\"\"), paste0(x198to238)), # delete from add'l cols\n                          x239to280 = ifelse(nchar(x53to197, type=\"chars\") > 0, paste0(\"\"), paste0(x239to280)), # delete from add'l cols\n                          x281to329 = ifelse(nchar(x53to197, type=\"chars\") > 0, paste0(\"\"), paste0(x281to329)), # delete from add'l cols\n                          x330to371 = ifelse(nchar(x53to197, type=\"chars\") > 0, paste0(\"\"), paste0(x330to371)), # delete from add'l cols\n                          x372to449 = ifelse(nchar(x53to197, type=\"chars\") > 0, paste0(\"\"), paste0(x372to449)), # delete from add'l cols\n                          x450toEnd = ifelse(nchar(x53to197, type=\"chars\") > 0, paste0(\"\"), paste0(x450toEnd)) # delete from add'l cols\n                          )\n\nallYearsDF_wide <- allYearsDF_wide %>% mutate_at(c(5,6), ~na_if(., '')) # replace blanks with NAs in the 2nd and 3rd cols. I tried this using paste in the previous mutate, but the result was a text string \"NA\".\n\n  # label each action with its call reason\n\nallYearsDF_wide <- fill(allYearsDF_wide, x5to52, .direction = \"down\")\n\nallYearsDF_wide <- allYearsDF_wide %>% \n  mutate(x53to197 = ifelse(row_number() == 1, \"Action\", x53to197))\n\ncolnames(allYearsDF_wide)[-c(1:4)] <- allYearsDF_wide[1,-c(1:4)]\n\nallYearsDF_wide <- allYearsDF_wide %>% \n  rename_with(tolower) %>%\n  rename_with(~ gsub(\"._\", \"_\", .x, fixed = TRUE)) %>%\n  rename_with(~ gsub(\"@\", \"at\", .x, fixed = TRUE)) %>%\n\n# allYearsDF_wide <- allYearsDF_wide %>%\n  filter(innerpageid != 1 | y != 69 | call_reason != \"Call_Reason\") %>%\n  filter(self != \"Self_Init\") %>%\n  # sep filename into town and year\n  separate_wider_regex(filename, c(town = \"^[A-Za-z]+\", year = \"[0-9]{4}$\")) %>%\n  mutate(town = factor(town),\n         year = make_date(year = as.integer(year), month = 1, day = 1)) %>%\n  select(-c(total, \"___%\")) %>%\n  mutate(across(call_reason:avg_time_at_scene, ~na_if(., \"\")))\n\n  # First df achievement unlocked:\n\ncallReasonsDF <- allYearsDF_wide %>% \n  filter(is.na(action)) %>% \n  filter(!grepl(\"total\", call_reason, ignore.case = TRUE)) %>%\n  filter(!is.na(self)) %>%\n  select(town, year, call_reason, self, disp, avg_arrive, avg_time_at_scene) %>%\n  mutate(call_reason = factor(call_reason)) %>%\n  mutate_at(c(\"self\", \"disp\"), as.integer) %>%\n  mutate_at(c(\"avg_arrive\", \"avg_time_at_scene\"), as.numeric) %>%\n  mutate(avg_arrive = ifelse(is.na(avg_arrive), 0, avg_arrive)) \n\n  # Second df achievement unlocked\n\ntotalReasonsDF <- allYearsDF_wide %>% \n  filter(is.na(action)) %>%\n  filter(grepl(\"^TOTAL$\", call_reason, ignore.case = FALSE)) %>%\n  select(c(town, year, self, disp, avg_arrive, avg_time_at_scene)) %>%\n  mutate_at(c(\"self\", \"disp\"), as.integer) %>%\n  mutate_at(c(\"avg_arrive\", \"avg_time_at_scene\"), as.numeric)\n\n  # Third df achievement unlocked\n\ncallActionsDF <- allYearsDF_wide %>% \n  filter(!is.na(action)) %>%\n  filter(!grepl(\"total\", call_reason, ignore.case = TRUE)) %>%\n  select(c(town, year, call_reason, action))  %>%\n  separate_wider_regex(action, c(actionname = \".+\", \" = \", actioncount = \".*\")) %>%\n  mutate(call_reason = factor(call_reason),\n         actionname = factor(actionname),\n         actioncount = str_squish(actioncount),\n         actioncount = as.integer(actioncount))\n\n  # Fourth df achievement unlocked\n\ntotalActionsDF <- allYearsDF_wide %>%\n  filter(is.na(action), is.na(self)) %>%\n  filter(!grepl(\"total\", call_reason, ignore.case = TRUE)) %>%\n  select(c(town, year, call_reason, disp)) %>%\n  separate_wider_regex(call_reason, c(actionname = \"[A-Za-z/()\\\\-\\\\s]+\", self = \"\\\\s[0-9]{1,4}\", \".*\")) %>%\n  mutate(actionname = factor(actionname),\n         self = str_squish(self),\n         disp = str_squish(disp)) %>%\n  mutate_at(c(\"self\", \"disp\"), as.integer)\n\n  # Fifth and final df achievement unlocked\n\noperatorSummaryDF <- allYearsDF_wide %>% \n  filter(!is.na(action)) %>%\n  filter(grepl(\"total\", call_reason, ignore.case = TRUE)) %>%\n  mutate(opcategory = str_extract(action, \"Sex|Race|Ethnicity\")) %>%\n  fill(opcategory) %>%\n  filter(!grepl(\"total\", action, ignore.case = TRUE)) %>%\n  rename(opid = action) %>%\n  select(c(town, year, opcategory, opid))  %>%\n  separate_wider_regex(opid, c(opid = \"[A-Za-z/\\\\-\\\\s]+\", opcount = \"\\\\s[0-9]{1,3}\", \".+\")) %>%\n  mutate(opcategory = factor(opcategory),\n         opid = str_squish(opid),\n         opid = factor(opid),\n         opcount = str_squish(opcount),\n         opcount = as.integer(opcount))\n```\n:::\n\n\n\n### Data Dictionary\n\nThese are the final data frame variables: \n\n::: {.callout-note icon=false}\n\n## callReasonsDF\n\n- call_reason: factor with 121 levels, describes the type of incident that resulted in a call to LPD or Shelburne Dispatch\n- self: the number of calls per call_reason made directly to LPD (or WPD before 2021)\n- disp: the number of calls per call_reason made to Shelburne Dispatch\n- avg_arrive: time in minutes between call received and when the officer arrived on the reported location or 'scene' of the incident\n- avg_time_at_scene: time in minutes between officer arrival and an outcome action (not necessarily the completion of the call itself -- calls may result in one or more new calls, referrals to a third party, an ongoing investigation, or a call that remains open and unresolved with no further action.)\n:::\n\n::: {.callout-note icon=false}\n\n## totalReasonsDF\n\nFetches totals of numeric variables from the report. This is only for quality-assurance comparison with calculated totals of **callReasonsDF** variables.\n\n:::\n\n::: {.callout-note icon=false}\n\n## callActionsDF\n\n- *call_reason*: Factor with 121 levels, same as in callReasonsDF\n- *actionname*: Factor with 47 levels, one for each action outcome for a call\n- *actioncount*: Count of actionnames for each call_reason\n:::\n\n::: {.callout-note icon=false}\n\n## totalActionsDF\n\nFetches totals from the *Call Action Breakdown* table. This supports quality-assurance comparison with calculated totals of **callActionsDF** variables. However the *Call Action Breakdown* table also attributes action counts to calls directly to the local police department and to Shelburne Dispatch. This information is not available in the *Call Reason Breakdown* table.\n\n- *self*: Count of actionnames for calls made directly to LPD (or WPD before 2021).\n- *disp*: Count of actionnames for calls made to Shelburne Dispatch.\n\n:::\n\n::: {.callout-note icon=false}\n## operatorSummaryDF\n\n- *opcategory*: Factor with 3 levels: Ethnicity, Race, or Sex\n- *opid*: Factor with 11 levels corresponding to specific ethnicity, races, sexes\n- *opcount*: Count of vehicle operators for each opid\n:::\n\n::: {.callout-note icon=false}\n\n## Key variables\n\nEach data frame includes the following variables that identify when and where the incident and response happened:\n\n- *town*: 2-level factor, Leverett or Wendell\n- *year*: date, 2018 to 2022 (in all cases, month = 1 and day = 1)\n\nI can join reason and action data by using town, year, and call_reason as a compound key.\n:::\n\n### Analysis Plan\n\nMy goal is to analyze and visualize call data in answer to this question: \n\n::: {.callout-warning icon=false}\n\n## The top question\n\n**What is the nature of the public safety need behind the increase in calls?**\n\n:::\n\nList more specific questions that help you to answer the general question mentioned in the above introduction, and specify the corresponding columns/variables you expect to use in the dataset.\nAnalysis plan: briefly explain what types of analysis or visualization you are conducting, and why you choose to conduct it (in other words, why such visualization/specific graphs help to answer the question). Some of the details of your plan may more naturally fit into the results section.\n\n### Descriptive Statistics \n- see final project assignment#1\n(CODE) Present the descriptive information of the dataset(s). \n(STORY) Select and describe only the variables of interest (that you will use in the later analysis and visualization): what do they measure (or what do the variable names mean), how are they measured (or what do the values mean), and what are their the maxes, means, and medians (if applicable) \nDon’t include tables or figures or statistics just to show you know how to use R - this is one of the most common mistakes students make. Your reader will be bored and the information is really not necessary unless it fits into the flow of your story in a critical way. You may be able to place the majority of descriptive statistics into an appendix if they don’t fit naturally into the flow of your text. Only include statistics, tables, and graphs that are clearly described in the text and fit into your overall story - any information that is only for reference shouldn’t play a prominent role in your storytelling flow.\nThis section should be collapsed into the data description when possible, leaving a more seamless flow from Analysis Plan to Results.\n\n\n### Results: Analysis and Visualization\n(STORY) Any additional analysis details, and perhaps a backreference to your analysis plan from section 5. (not needed if section 6 is collapsed with sections 2-4)\n(CODE-Optional) Statistical Analyses: see “Role of Statistics”\n(CODE) Visualization\nYou should organize your graphs and pictures by a small set of topics/questions\nThere is no strict requirement on the number of graphs/figures you should make. However, we expect you to present sufficient graphs that are necessary to answer the question.\n(STORY) Interpret your analyses and visualization results for the reader\nWhat information do the analysis results or graphs tell you?\nYou should describe the information of graphs comprehensively and detailed enough for readers to picture or envision the graphs in their brains.\n\nhttps://dacss.github.io/601_Fall_2022_final_posts/posts/final_project_Guanhua_Tan.html#painting-a-global-map\n\n### Conclusion and Discussion\n(STORY) Conclusion and Discussion \nA brief summary of your work and your findings. What is (are) the answer(s) to your question?\n(optional) Discuss the limitation of your analyses and visualization; and what further analyses or visualizations you would conduct in the future\n\n\n### Bibliography\nHint: at minimum, you should cite the source of the dataset and R as a programming language.\n\t- Any citation style is appropriate since students come from diverse academic backgrounds. Please use whichever style is appropriate for your work.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n######### all good up to this point.\n######### sanity tests\n######### make more pipelines\n\n### rubrick items\n# 1 \n# Communication suitable for an expert and general audience\n# 2\n# Data cleaning and recoding choices and justification\n# 3\n# Data driven story-telling (using data to answer questions)\n# 4\n# Effective and efficient coding\n\n\n\n## testers\nallYearsDF_wideACTION <- arrange(allYearsDF_wide, Action)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `arrange()`:\nℹ In argument: `..1 = Action`.\nCaused by error:\n! object 'Action' not found\n```\n:::\n\n```{.r .cell-code}\nallYearsDF_wideSELF <- arrange(allYearsDF_wide, desc(Self)) # checks out \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `arrange()`:\nℹ In argument: `..1 = Self`.\nCaused by error:\n! object 'Self' not found\n```\n:::\n\n```{.r .cell-code}\n# sanity check ... pass!\ncallReasonsDF %>%\n  group_by(town) %>%\n  summarise(selfSum = sum(as.numeric(self)), dispSum = sum(as.numeric(disp)), arriveSum = sum(as.numeric(avg_arrive)), sceneSum = sum(as.numeric(avg_time_at_scene)), .groups = 'drop')\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"town\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"selfSum\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"dispSum\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"arriveSum\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sceneSum\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Leverett\",\"2\":\"11498\",\"3\":\"2716\",\"4\":\"2308.54\",\"5\":\"10025.24\"},{\"1\":\"Wendell\",\"2\":\"1063\",\"3\":\"1387\",\"4\":\"3282.88\",\"5\":\"6288.43\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}